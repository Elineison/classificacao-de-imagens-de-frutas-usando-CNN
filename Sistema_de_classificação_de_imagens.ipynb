{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Classificação de Imagens de Frutas usando CNN**\n",
        "\n",
        "Neste projeto, temos um sistema para classificar imagens em três categorias: maçã, banana ou laranja.Utilizaremos uma Rede Neural Convolucional (CNN) para essa tarefa, pois ela é muito eficaz para extrair características visuais importantes das imagens.\n",
        "\n",
        "Antes de iniciar o treinamento do modelo, você precisará enviar manualmente os arquivos compactados do dataset filtrado \"fruits_filtered.zip\" e a pasta \"test_images\" para o Colab. Esses arquivos contem apenas as pastas das três classes de frutas desejadas e estão com imagens separadas para treinamento e teste, achei que assim tornaria mais fácil de ser testado, pois o dataset original que utilizei para a realização da oficina era muito grande porque haviam diversas classes. A fonte do dataset original utilizado: https://www.kaggle.com/datasets/moltean/fruits?resource=download. Vou deixar instruções no codigo\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-fbZLy5PIPfs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B896s_PNIGir"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "# Faz upload dos dois arquivos ZIP de uma vez:\n",
        "uploaded = files.upload()\n",
        "# O professor selecionará os arquivos \"fruits_filtered.zip\" e \"test_images.zip\"\n",
        "\n",
        "# Extraindo o dataset filtrado (fruits_filtered.zip)\n",
        "zip_path_dataset = 'fruits_filtered.zip'\n",
        "extract_path_dataset = 'fruits_filtered'\n",
        "if not os.path.exists(extract_path_dataset):\n",
        "    with zipfile.ZipFile(zip_path_dataset, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_path_dataset)\n",
        "    print(\"Dataset filtrado extraído com sucesso!\")\n",
        "else:\n",
        "    print(\"O dataset filtrado já está extraído.\")\n",
        "\n",
        "# Extraindo as imagens de teste (test_images.zip)\n",
        "zip_path_test = 'test_images.zip'\n",
        "extract_path_test = 'test_images'\n",
        "if not os.path.exists(extract_path_test):\n",
        "    with zipfile.ZipFile(zip_path_test, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_path_test)\n",
        "    print(\"Pasta de teste extraída com sucesso!\")\n",
        "else:\n",
        "    print(\"A pasta de teste já está extraída.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1 - Indicação clara do tipo de rede neural usada e justificativa do porquê de ser a escolha mais adequada;**\n",
        "\n",
        "Resposta:\n",
        "\n",
        "Utilizaremos uma Rede Neural Convolucional (CNN) para este projeto. Como as CNNs são especialmente projetadas para tarefas de visão computacional, pois elas conseguem identificar automaticamente padrões e características visuais em imagens, como bordas, texturas, formas e núcleos. Essa capacidade de extração de características permite que a rede destaque os elementos essenciais para diferenciar entre as classes de frutas (maçã, banana e laranja). As camadas de pooling das CNNs ajudam a reduzir a dimensionalidade dos dados sem perder informações importantes, o que torna o modelo mais eficiente e menos suscetível ao overfitting. Por todas essas razões, a CNN é a escolha ideal para resolver o problema de classificação proposta no trabalho.\n"
      ],
      "metadata": {
        "id": "GM7ZOzQIJXSW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importação das bibliotecas\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "6drJvmayKCLF"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2 - Descrição do processo de pré-processamento das imagens**\n",
        "\n",
        "Resposta:\n",
        "\n",
        "No pré-processamento, as imagens passam por três etapas principais:\n",
        "\n",
        "Redimensionamento: É feito um ajuste para que todas as imagens tenham o tamanho de 128×128 pixels, mantendo um padrão.\n",
        "\n",
        "Normalização: Os valores dos pixels são divididos por 255, transformando-os para a faixa [0, 1]. Isso ajuda o modelo a processar os dados de forma mais eficiente.\n",
        "\n",
        "Data Augmentation: São aplicadas transformações como rotação, zoom, translação e inversão horizontal para aumentar a diversidade dos dados e reduzir o overfitting, garantindo que o modelo generalize bem."
      ],
      "metadata": {
        "id": "0p_zVSZlKVdR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Configurando o gerador de dados com normalização e data augmentation\n",
        "datagen = ImageDataGenerator(\n",
        "    rescale=1./255,           # Normalização para [0,1]\n",
        "    validation_split=0.2,     # Reserva 20% dos dados para validação para manter o padrão 80/20\n",
        "    rotation_range=20,\n",
        "    zoom_range=0.15,\n",
        "    horizontal_flip=True\n",
        ")\n",
        "\n",
        "# Carrega as imagens do dataset filtrado extraído\n",
        "train_data = datagen.flow_from_directory(\n",
        "    extract_path_dataset,      # Caminho para a pasta \"fruits_filtered\"\n",
        "    target_size=(128, 128),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical',\n",
        "    subset='training'\n",
        ")\n",
        "\n",
        "validation_data = datagen.flow_from_directory(\n",
        "    extract_path_dataset,\n",
        "    target_size=(128, 128),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical',\n",
        "    subset='validation'\n",
        ")"
      ],
      "metadata": {
        "id": "dZW8m-ZjK1Uj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8234d00e-8e02-4643-acc2-4f3ad6741c10"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 7592 images belonging to 3 classes.\n",
            "Found 1896 images belonging to 3 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3 - Apresentação do modelo de rede neural em Python, com uma breve explicação de cada camada**\n",
        "\n",
        "Resposta:\n",
        "\n",
        "O modelo é uma CNN constituída dos seguintes blocos:\n",
        "\n",
        "Conv2D: Essa camada analisa a imagem e extrai detalhes essenciais, como bordas e texturas.\n",
        "\n",
        "MaxPooling2D: Em seguida, a camada de pooling reduz a quantidade de dados, mantendo as características mais importantes.\n",
        "\n",
        "Flatten: Transforma as informações extraídas em uma lista única (vetor unidimensional), que pode ser usada para a classificação.\n",
        "\n",
        "Dense: Camadas totalmente conectadas que, com base no vetor, realizam a classificação final.\n",
        "\n",
        "Dropout: Uma camada que ajuda a evitar o overfitting, desligando aleatoriamente alguns neurônios durante o treinamento."
      ],
      "metadata": {
        "id": "gmKzq6C0LIY-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "\n",
        "# Construindo o modelo CNN\n",
        "model = Sequential()\n",
        "\n",
        "# Primeira camada convolucional com 32 filtros (3x3) e pooling\n",
        "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(128, 128, 3)))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "\n",
        "# Segunda camada convolucional com 64 filtros e pooling\n",
        "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "\n",
        "# Terceira camada convolucional com 128 filtros e pooling\n",
        "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "\n",
        "# Convertendo os mapas de características em um vetor unidimensional\n",
        "model.add(Flatten())\n",
        "\n",
        "# Camada densa com 128 neurônios e dropout para evitar overfitting\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "# Camada de saída: 3 neurônios (apple, banana, orange) com softmax para classificar\n",
        "model.add(Dense(3, activation='softmax'))\n",
        "\n",
        "# Mostrando um resumo da arquitetura do modelo\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "ElsRO-0vLfO4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476
        },
        "outputId": "b5c574a3-c525-436a-b619-03e02f054cb7"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ conv2d_3 (\u001b[38;5;33mConv2D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m126\u001b[0m, \u001b[38;5;34m126\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │             \u001b[38;5;34m896\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ max_pooling2d_3 (\u001b[38;5;33mMaxPooling2D\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m63\u001b[0m, \u001b[38;5;34m63\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ conv2d_4 (\u001b[38;5;33mConv2D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m61\u001b[0m, \u001b[38;5;34m61\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │          \u001b[38;5;34m18,496\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ max_pooling2d_4 (\u001b[38;5;33mMaxPooling2D\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ conv2d_5 (\u001b[38;5;33mConv2D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │          \u001b[38;5;34m73,856\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ max_pooling2d_5 (\u001b[38;5;33mMaxPooling2D\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ flatten_1 (\u001b[38;5;33mFlatten\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m25088\u001b[0m)               │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │       \u001b[38;5;34m3,211,392\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)                   │             \u001b[38;5;34m387\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ conv2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">126</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">126</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ max_pooling2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">63</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">63</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ conv2d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">61</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">61</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │          <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ max_pooling2d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ conv2d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ max_pooling2d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ flatten_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25088</span>)               │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │       <span style=\"color: #00af00; text-decoration-color: #00af00\">3,211,392</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">387</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m3,305,027\u001b[0m (12.61 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,305,027</span> (12.61 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m3,305,027\u001b[0m (12.61 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,305,027</span> (12.61 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **4 - Compilação e treinamento do modelo, mencionando a função de perda e o otimizador usados**\n",
        "\n",
        "Resposta:\n",
        "\n",
        "O modelo é compilado com o otimizador adam que é muito usado por sua eficiência e rapidez e a função de perda categorical_crossentropy, adequada para problemas com várias classes. Em seguida, o treinamento é realizado usando os dados de treinamento e validação, permitindo acompanhar a evolução do modelo ao longo das épocas."
      ],
      "metadata": {
        "id": "I1MKBHGtLrrq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "k-beldv8NsSV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compilando o modelo\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Treinamento do modelo\n",
        "history = model.fit(\n",
        "    train_data,\n",
        "    epochs=20,        # Número de épocas (pode ser alterado conforme necessário)\n",
        "    validation_data=validation_data\n",
        ")"
      ],
      "metadata": {
        "id": "GjaLPpGeNPMq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "483f6d99-362d-491f-cb50-39f119a7d383"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m238/238\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m160s\u001b[0m 665ms/step - accuracy: 0.9172 - loss: 0.2354 - val_accuracy: 1.0000 - val_loss: 0.0012\n",
            "Epoch 2/20\n",
            "\u001b[1m238/238\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m160s\u001b[0m 671ms/step - accuracy: 0.9983 - loss: 0.0060 - val_accuracy: 0.9989 - val_loss: 0.0019\n",
            "Epoch 3/20\n",
            "\u001b[1m238/238\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m155s\u001b[0m 652ms/step - accuracy: 0.9967 - loss: 0.0108 - val_accuracy: 0.9931 - val_loss: 0.0129\n",
            "Epoch 4/20\n",
            "\u001b[1m238/238\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m158s\u001b[0m 664ms/step - accuracy: 0.9976 - loss: 0.0055 - val_accuracy: 1.0000 - val_loss: 7.9720e-04\n",
            "Epoch 5/20\n",
            "\u001b[1m238/238\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m161s\u001b[0m 677ms/step - accuracy: 0.9989 - loss: 0.0021 - val_accuracy: 0.9968 - val_loss: 0.0179\n",
            "Epoch 6/20\n",
            "\u001b[1m238/238\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m155s\u001b[0m 651ms/step - accuracy: 0.9996 - loss: 0.0024 - val_accuracy: 0.9979 - val_loss: 0.0075\n",
            "Epoch 7/20\n",
            "\u001b[1m238/238\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m157s\u001b[0m 660ms/step - accuracy: 1.0000 - loss: 3.0601e-04 - val_accuracy: 1.0000 - val_loss: 5.8909e-05\n",
            "Epoch 8/20\n",
            "\u001b[1m238/238\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m159s\u001b[0m 670ms/step - accuracy: 1.0000 - loss: 1.1112e-04 - val_accuracy: 1.0000 - val_loss: 3.7109e-05\n",
            "Epoch 9/20\n",
            "\u001b[1m238/238\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m158s\u001b[0m 662ms/step - accuracy: 1.0000 - loss: 2.4639e-04 - val_accuracy: 0.9963 - val_loss: 0.0084\n",
            "Epoch 10/20\n",
            "\u001b[1m238/238\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m156s\u001b[0m 655ms/step - accuracy: 0.9951 - loss: 0.0159 - val_accuracy: 0.9947 - val_loss: 0.0226\n",
            "Epoch 11/20\n",
            "\u001b[1m238/238\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m154s\u001b[0m 647ms/step - accuracy: 0.9993 - loss: 0.0018 - val_accuracy: 1.0000 - val_loss: 3.5186e-04\n",
            "Epoch 12/20\n",
            "\u001b[1m238/238\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m161s\u001b[0m 676ms/step - accuracy: 0.9943 - loss: 0.0219 - val_accuracy: 1.0000 - val_loss: 3.3281e-04\n",
            "Epoch 13/20\n",
            "\u001b[1m238/238\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m155s\u001b[0m 651ms/step - accuracy: 1.0000 - loss: 2.4851e-04 - val_accuracy: 0.9989 - val_loss: 0.0016\n",
            "Epoch 14/20\n",
            "\u001b[1m238/238\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m162s\u001b[0m 682ms/step - accuracy: 0.9986 - loss: 0.0033 - val_accuracy: 0.9963 - val_loss: 0.0142\n",
            "Epoch 15/20\n",
            "\u001b[1m238/238\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m153s\u001b[0m 643ms/step - accuracy: 0.9997 - loss: 0.0017 - val_accuracy: 1.0000 - val_loss: 1.0197e-04\n",
            "Epoch 16/20\n",
            "\u001b[1m238/238\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m154s\u001b[0m 648ms/step - accuracy: 0.9997 - loss: 0.0011 - val_accuracy: 1.0000 - val_loss: 3.0833e-05\n",
            "Epoch 17/20\n",
            "\u001b[1m238/238\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m156s\u001b[0m 656ms/step - accuracy: 0.9736 - loss: 0.1672 - val_accuracy: 0.9979 - val_loss: 0.0098\n",
            "Epoch 18/20\n",
            "\u001b[1m238/238\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m161s\u001b[0m 679ms/step - accuracy: 0.9953 - loss: 0.0137 - val_accuracy: 0.9979 - val_loss: 0.0032\n",
            "Epoch 19/20\n",
            "\u001b[1m238/238\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m162s\u001b[0m 681ms/step - accuracy: 0.9980 - loss: 0.0038 - val_accuracy: 0.9995 - val_loss: 0.0023\n",
            "Epoch 20/20\n",
            "\u001b[1m238/238\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m154s\u001b[0m 648ms/step - accuracy: 0.9980 - loss: 0.0030 - val_accuracy: 0.9989 - val_loss: 0.0022\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **5 - Processo de avaliação e validação do modelo**\n",
        "\n",
        "Resposta:\n",
        "\n",
        "Após o treinamento, o desempenho do modelo é avaliado com o conjunto de validação. A função evaluate retorna as métricas de loss e acurácia, permitindo verificar se o modelo está generalizando bem para novos dados.\n",
        "\n"
      ],
      "metadata": {
        "id": "IgpchHUwwteq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Avaliando o modelo com os dados de validação\n",
        "loss, accuracy = model.evaluate(validation_data)\n",
        "print(f\"Loss: {loss:.4f}, Accuracy: {accuracy:.4f}\")"
      ],
      "metadata": {
        "id": "9KzwRynFsDGZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d07f8f9-6dea-405f-9960-edc867dd7b22"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 223ms/step - accuracy: 0.9994 - loss: 5.9495e-04\n",
            "Loss: 0.0005, Accuracy: 0.9995\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **6 - Conclusão explicando como o modelo resolverá o problema proposto**\n",
        "\n",
        "Resposta:\n",
        "\n",
        "A CNN criada consegue reconhecer automaticamente os detalhes que diferenciam maçãs, bananas e laranjas. Quando uma nova imagem é enviada, o sistema redimensiona, normaliza e processa a imagem para extrair suas características importantes. Com base nisso, a camada de saída calcula a probabilidade para cada classe, permitindo a classificação automática e precisa da imagem."
      ],
      "metadata": {
        "id": "IS7mt-4txEoX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Teste Interativo – Avaliando uma imagem inédita**\n",
        "\n",
        "Instrução:\n",
        "Para testar o sistema com uma imagem que o modelo nunca viu, existem duas opções:\n",
        "\n",
        "Opção A: Selecionar uma imagem presente na pasta \"test_images\" e renomear 'img_path' com o caminho do arquivo que deseja testar. Exemplo: test_images/apple/186_100.jpg\n",
        "\n",
        "Opção B: Fazer upload manual de uma nova imagem."
      ],
      "metadata": {
        "id": "OW6EpPe7xsdm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Exemplo opção A\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "import numpy as np\n",
        "\n",
        "# Definindo o caminho para a imagem (certifique-se de que o arquivo exista)\n",
        "img_path = 'test_images/pasta(apple ou banana ou orange)/nome_do_arquivo.jpg'\n",
        "# Substitua indicando o caminho correto da imagem a ser testada\n",
        "\n",
        "# Carrega a imagem com o tamanho esperado (128x128 pixels)\n",
        "test_img = load_img(img_path, target_size=(128, 128))\n",
        "test_img_array = img_to_array(test_img)\n",
        "test_img_array = np.expand_dims(test_img_array, axis=0)\n",
        "test_img_array /= 255.0  # Normalização\n",
        "\n",
        "# Obtém o mapeamento dos índices para os nomes das classes a partir dos dados de treino\n",
        "class_indices = train_data.class_indices\n",
        "reverse_class_indices = {v: k for k, v in class_indices.items()}\n",
        "\n",
        "# Realiza a predição\n",
        "prediction = model.predict(test_img_array)\n",
        "predicted_class_index = np.argmax(prediction, axis=1)[0]\n",
        "predicted_class_name = reverse_class_indices[predicted_class_index]\n",
        "\n",
        "print(\"Probabilidades para cada classe:\", prediction)\n",
        "print(\"Classe predita (índice):\", predicted_class_index)\n",
        "print(\"Classe predita (nome):\", predicted_class_name)"
      ],
      "metadata": {
        "id": "uz8EHvPQx6Pi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Exemplo opção B\n",
        "from google.colab import files\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "import numpy as np\n",
        "\n",
        "# Obtém o mapeamento dos índices para os nomes das classes\n",
        "class_indices = train_data.class_indices\n",
        "reverse_class_indices = dict((v, k) for k, v in class_indices.items())\n",
        "\n",
        "# Abre uma janela para fazer o upload de uma nova imagem\n",
        "uploaded_img = files.upload()\n",
        "\n",
        "# Seleciona a imagem carregada (assume-se que apenas uma imagem foi enviada)\n",
        "img_name = list(uploaded_img.keys())[0]\n",
        "print(\"Imagem carregada:\", img_name)\n",
        "\n",
        "# Carrega e pré-processa a imagem\n",
        "test_img = load_img(img_name, target_size=(128, 128))\n",
        "test_img_array = img_to_array(test_img)\n",
        "test_img_array = np.expand_dims(test_img_array, axis=0)\n",
        "test_img_array /= 255.0  # Normalização\n",
        "\n",
        "# Realiza a predição\n",
        "prediction = model.predict(test_img_array)\n",
        "predicted_class_index = np.argmax(prediction, axis=1)[0]\n",
        "predicted_class_name = reverse_class_indices[predicted_class_index]\n",
        "\n",
        "print(\"Probabilidades:\", prediction)\n",
        "print(\"Classe predita (índice):\", predicted_class_index)\n",
        "print(\"Classe predita (nome):\", predicted_class_name)"
      ],
      "metadata": {
        "id": "9wz30J8kyL3w",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 160
        },
        "outputId": "b8bfa78f-11d9-48d3-bb63-b7ace91dadc1"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-51d716ea-4ff4-46a2-8895-8b70422d78a8\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-51d716ea-4ff4-46a2-8895-8b70422d78a8\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving r_327_100.jpg to r_327_100.jpg\n",
            "Imagem carregada: r_327_100.jpg\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step\n",
            "Probabilidades: [[1.4410609e-06 1.3696231e-15 9.9999857e-01]]\n",
            "Classe predita (índice): 2\n",
            "Classe predita (nome): orange\n"
          ]
        }
      ]
    }
  ]
}